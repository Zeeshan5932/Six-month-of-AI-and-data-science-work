{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author : `Muhammad Abdullah`\n",
    "Email : `mrabdullah0303@gmail.com`\n",
    "---\n",
    "About This Notebook : In this Notebook i will Code a Program to Select best Model and Its Parameters by HyperparameteringTunning . \n",
    "I used `Tips` and `Diamonds` Dataset in This Notebook . \n",
    "\n",
    "U can Ask Questions Related to this Notebook . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraires \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Train test Split \n",
    "from sklearn.model_selection import train_test_split\n",
    "# Models \n",
    "from sklearn.naive_bayes import GaussianNB , BernoulliNB , MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier , DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier , AdaBoostClassifier ,RandomForestRegressor , AdaBoostRegressor\n",
    "from xgboost import XGBClassifier , XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression ,LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor , KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor , GradientBoostingClassifier\n",
    "from sklearn.svm import SVC , SVR\n",
    "#metrics\n",
    "from sklearn.metrics import mean_squared_error , mean_absolute_error , r2_score , classification_report , accuracy_score , f1_score , precision_score\n",
    "#import grid search cv for cross validation\n",
    "from sklearn.model_selection import GridSearchCV , RandomizedSearchCV\n",
    "# import preprocessors\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer , PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Remove Warning \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Saving Model \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset \n",
    "df = sns.load_dataset('tips')\n",
    "# select features and variables\n",
    "X = df.drop('tip', axis=1)\n",
    "y = df['tip']\n",
    "\n",
    "# label encode Input  variables\n",
    "le = LabelEncoder()\n",
    "X['sex'] = le.fit_transform(X['sex'])\n",
    "X['smoker'] = le.fit_transform(X['smoker'])\n",
    "X['day'] = le.fit_transform(X['day'])\n",
    "X['time'] = le.fit_transform(X['time'])\n",
    "\n",
    "# # Quantile Transform , PowerTransformer\n",
    "# qt = QuantileTransformer(output_distribution='normal', random_state=42, n_quantiles=500)\n",
    "# X['total_bill'] = qt.fit_transform(X[['total_bill']])\n",
    "\n",
    "# #Standard Scaler\n",
    "# scaler = StandardScaler()\n",
    "# X['total_bill'] = scaler.fit_transform(X[['total_bill']])\n",
    "\n",
    "# split the data into train and test data with 80% training dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 RandomiedSearchCv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "R_2 for SVR is -0.17\n",
      "R_2 for RandomForestRegressor is  0.23\n",
      "R_2 for AdaBoostRegressor is  0.23\n",
      "R_2 for DecisionTreeRegressor is  0.30\n",
      "R_2 for GradientBoostingRegressor is  0.35\n",
      "R_2 for LinearRegression is  0.44\n",
      "R_2 for KNeighborsRegressor is  0.47\n",
      "R_2 for XGBRegressor is  0.47\n",
      "\n",
      "\n",
      "MAE for SVR is  0.89\n",
      "MAE for RandomForestRegressor is  0.78\n",
      "MAE for AdaBoostRegressor is  0.78\n",
      "MAE for DecisionTreeRegressor is  0.72\n",
      "MAE for GradientBoostingRegressor is  0.77\n",
      "MAE for LinearRegression is  0.67\n",
      "MAE for KNeighborsRegressor is  0.62\n",
      "MAE for XGBRegressor is  0.65\n",
      "\n",
      "\n",
      "MSE for SVR is  1.46\n",
      "MSE for RandomForestRegressor is  0.96\n",
      "MSE for AdaBoostRegressor is  0.96\n",
      "MSE for DecisionTreeRegressor is  0.88\n",
      "MSE for GradientBoostingRegressor is  0.81\n",
      "MSE for LinearRegression is  0.69\n",
      "MSE for KNeighborsRegressor is  0.66\n",
      "MSE for XGBRegressor is  0.66\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Create a dictionaries of list of models to evaluate performance with hyperparameters\n",
    "models = { \n",
    "          'LinearRegression' : (LinearRegression(), {}),\n",
    "          'SVR' : (SVR(), {'kernel': ['rbf', 'poly', 'sigmoid']}),\n",
    "          'DecisionTreeRegressor' : (DecisionTreeRegressor(random_state=42), {'max_depth': [None, 5, 10]}),\n",
    "          'RandomForestRegressor' : (RandomForestRegressor(random_state=42), {'n_estimators': [10, 100]}),\n",
    "          'KNeighborsRegressor' : (KNeighborsRegressor(), {'n_neighbors': np.arange(3, 100, 2)}),\n",
    "          'GradientBoostingRegressor' : (GradientBoostingRegressor(random_state=42),{'n_estimators': [10, 100]}),\n",
    "          'XGBRegressor' : (XGBRegressor(), {'n_estimators': [10, 100]}),  \n",
    "          'AdaBoostRegressor': (AdaBoostRegressor(random_state=42), {'n_estimators': [10, 100]}),        \n",
    "          }\n",
    "model_scores = []\n",
    "# For loop to iterate over the models\n",
    "for name, (model, params) in models.items():\n",
    "    # create a pipline\n",
    "    pipeline = RandomizedSearchCV(model, params, cv=5 , verbose=1 , n_jobs=-1)\n",
    "    \n",
    "    # fit the pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # make prediction from each model\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    # Metric\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    best_parameter = pipeline.best_params_\n",
    "    model_scores.append((name,r2 , mae , mse,best_parameter))\n",
    "# selecting the best model from all above models with evaluation metrics sorting method\n",
    "sorted_models = sorted(model_scores, key=lambda x: x[1], reverse=False)\n",
    "# # Printing Each model with evaluation metrics\n",
    "for model in sorted_models:\n",
    "    print('R_2 for', f\"{model[0]} is {model[1]: .2f}\")\n",
    "print('\\n')\n",
    "for model in sorted_models:\n",
    "    print('MAE for', f\"{model[0]} is {model[2]: .2f}\")\n",
    "print('\\n')\n",
    "for model in sorted_models:\n",
    "    print('MSE for', f\"{model[0]} is {model[3]: .2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best Parameter and Best Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for SVR is {'kernel': 'poly'}\n",
      "Best Parameters for RandomForestRegressor is {'n_estimators': 100}\n",
      "Best Parameters for AdaBoostRegressor is {'n_estimators': 10}\n",
      "Best Parameters for DecisionTreeRegressor is {'max_depth': 5}\n",
      "Best Parameters for GradientBoostingRegressor is {'n_estimators': 10}\n",
      "Best Parameters for LinearRegression is {}\n",
      "Best Parameters for XGBRegressor is {'n_estimators': 10}\n",
      "Best Parameters for KNeighborsRegressor is {'n_neighbors': 55}\n",
      "\n",
      "\n",
      "Best model based on R2 is KNeighborsRegressor with R2 of 0.49\n",
      "Best Parameters: {'n_neighbors': 55}\n",
      "Best model based on MAE is XGBRegressor with MAE of 0.65\n",
      "Best Parameters: {'n_estimators': 10}\n",
      "Best model based on MSE is KNeighborsRegressor with MSE of 0.63\n",
      "Best Parameters: {'n_neighbors': 55}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Best Parameter for each model\n",
    "for model in sorted_models:\n",
    "    print('Best Parameters for', f\"{model[0]} is {model[4]}\")\n",
    "print('\\n')\n",
    "# Selecting the best model based on R2\n",
    "best_r2_model = max(model_scores, key=lambda x: x[1])\n",
    "print('Best model based on R2 is', f\"{best_r2_model[0]} with R2 of {best_r2_model[1]:.2f}\")\n",
    "print(f'Best Parameters: {best_r2_model[4]}')\n",
    "# Selecting the best model based on MAE\n",
    "best_mae_model = min(model_scores, key=lambda x: x[2])\n",
    "print('Best model based on MAE is', f\"{best_mae_model[0]} with MAE of {best_mae_model[2]:.2f}\")\n",
    "print(f'Best Parameters: {best_mae_model[4]}')\n",
    "# Selecting the best model based on MSE \n",
    "best_mse_model = min(model_scores, key=lambda x: x[3])\n",
    "print('Best model based on MSE is', f\"{best_mse_model[0]} with MSE of {best_mse_model[3]:.2f}\")\n",
    "print(f'Best Parameters: {best_mse_model[4]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Saving Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsRegressor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.11])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting the best model based on R2\n",
    "best_r2_model = max(model_scores, key=lambda x: x[1])\n",
    "best_model_name = best_r2_model[0]\n",
    "best_model_instance = models[best_model_name][0]\n",
    "print(best_model_name)\n",
    "# export \n",
    "pickle.dump(best_model_instance,open('pipe.pkl','wb'))\n",
    "# Load Model \n",
    "svr = pickle.load(open('pipe.pkl','rb'))\n",
    "# Fit Model \n",
    "svr.fit(X_train, y_train)\n",
    "# Assume user input\n",
    "test_input2 = np.array([15, 1, 0, 3, 1, 1],dtype=object).reshape(1,6)\n",
    "# Predicting the output\n",
    "svr.predict(test_input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.5676944], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting the best model based on MAE\n",
    "best_mae_model = min(model_scores, key=lambda x: x[2])\n",
    "best_model_name = best_mae_model[0]\n",
    "best_model_instance = models[best_model_name][0]\n",
    "print(best_model_name)\n",
    "# export \n",
    "pickle.dump(best_model_instance,open('pipe.pkl','wb'))\n",
    "# Load Model \n",
    "svr = pickle.load(open('pipe.pkl','rb'))\n",
    "# Fit Model \n",
    "svr.fit(X_train, y_train)\n",
    "# Assume user input\n",
    "test_input2 = np.array([15, 1, 0, 3, 1, 1],dtype=object).reshape(1,6)\n",
    "# Predicting the output\n",
    "svr.predict(test_input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsRegressor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.11])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting the best model based on MSE\n",
    "best_mse_model = min(model_scores, key=lambda x: x[3])\n",
    "best_model_name = best_mse_model[0]\n",
    "best_model_instance = models[best_model_name][0]\n",
    "print(best_model_name)\n",
    "# export \n",
    "pickle.dump(best_model_instance,open('pipe.pkl','wb'))\n",
    "# Load Model \n",
    "svr = pickle.load(open('pipe.pkl','rb'))\n",
    "# Fit Model \n",
    "svr.fit(X_train, y_train)\n",
    "# Assume user input\n",
    "test_input2 = np.array([15, 1, 0, 3, 1, 1],dtype=object).reshape(1,6)\n",
    "# Predicting the output\n",
    "svr.predict(test_input2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using RandomizedCvSearch The Best Performing Model On the base of R2 and MAE  is `SVR` and One the Base of MSE `KNeighborsRegressor` is Performing Well  On `Diamonds` Dataset \n",
    "- `Best Parameters for SVR is {'kernel': 'poly', 'gamma': 0.01, 'epsilon': 0.001, 'C': 1}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 GridCvSearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_2 for SVR is -0.17\n",
      "R_2 for RandomForestRegressor is  0.23\n",
      "R_2 for AdaBoostRegressor is  0.23\n",
      "R_2 for DecisionTreeRegressor is  0.30\n",
      "R_2 for GradientBoostingRegressor is  0.35\n",
      "R_2 for LinearRegression is  0.44\n",
      "R_2 for KNeighborsRegressor is  0.47\n",
      "R_2 for XGBRegressor is  0.47\n",
      "\n",
      "\n",
      "MAE for SVR is  0.89\n",
      "MAE for RandomForestRegressor is  0.78\n",
      "MAE for AdaBoostRegressor is  0.78\n",
      "MAE for DecisionTreeRegressor is  0.72\n",
      "MAE for GradientBoostingRegressor is  0.77\n",
      "MAE for LinearRegression is  0.67\n",
      "MAE for KNeighborsRegressor is  0.62\n",
      "MAE for XGBRegressor is  0.65\n",
      "\n",
      "\n",
      "MSE for SVR is  1.46\n",
      "MSE for RandomForestRegressor is  0.96\n",
      "MSE for AdaBoostRegressor is  0.96\n",
      "MSE for DecisionTreeRegressor is  0.88\n",
      "MSE for GradientBoostingRegressor is  0.81\n",
      "MSE for LinearRegression is  0.69\n",
      "MSE for KNeighborsRegressor is  0.66\n",
      "MSE for XGBRegressor is  0.66\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Create a dictionaries of list of models to evaluate performance with hyperparameters\n",
    "models = { \n",
    "          'LinearRegression' : (LinearRegression(), {}),\n",
    "          'SVR' : (SVR(), {'kernel': ['rbf', 'poly', 'sigmoid']}),\n",
    "          'DecisionTreeRegressor' : (DecisionTreeRegressor(random_state=42), {'max_depth': [None, 5, 10]}),\n",
    "          'RandomForestRegressor' : (RandomForestRegressor(random_state=42), {'n_estimators': [10, 100]}),\n",
    "          'KNeighborsRegressor' : (KNeighborsRegressor(), {'n_neighbors': np.arange(3, 100, 2)}),\n",
    "          'GradientBoostingRegressor' : (GradientBoostingRegressor(random_state=42),{'n_estimators': [10, 100]}),\n",
    "          'XGBRegressor' : (XGBRegressor(), {'n_estimators': [10, 100]}),  \n",
    "          'AdaBoostRegressor': (AdaBoostRegressor(random_state=42), {'n_estimators': [10, 100]}),        \n",
    "          }\n",
    "\n",
    "model_scores = []\n",
    "# For loop to iterate over the models\n",
    "for name, (model, params) in models.items():\n",
    "    # create a pipline\n",
    "    pipeline = GridSearchCV(model, params, cv=5 , n_jobs=-1)\n",
    "    \n",
    "    # fit the pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # make prediction from each model\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    # Metric\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    best_parameter = pipeline.best_params_\n",
    "    model_scores.append((name,r2 , mae , mse,best_parameter))\n",
    "\n",
    "# selecting the best model from all above models with evaluation metrics sorting method\n",
    "sorted_models = sorted(model_scores, key=lambda x: x[1], reverse=False)\n",
    "\n",
    "# Printing Each model with evaluation metrics\n",
    "for model in sorted_models:\n",
    "    print('R_2 for', f\"{model[0]} is {model[1]: .2f}\")\n",
    "print('\\n')\n",
    "for model in sorted_models:\n",
    "    print('MAE for', f\"{model[0]} is {model[2]: .2f}\")\n",
    "print('\\n')\n",
    "for model in sorted_models:\n",
    "    print('MSE for', f\"{model[0]} is {model[3]: .2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BestParameter and Best Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for SVR is {'kernel': 'poly'}\n",
      "Best Parameters for RandomForestRegressor is {'n_estimators': 100}\n",
      "Best Parameters for AdaBoostRegressor is {'n_estimators': 10}\n",
      "Best Parameters for DecisionTreeRegressor is {'max_depth': 5}\n",
      "Best Parameters for GradientBoostingRegressor is {'n_estimators': 10}\n",
      "Best Parameters for LinearRegression is {}\n",
      "Best Parameters for KNeighborsRegressor is {'n_neighbors': 17}\n",
      "Best Parameters for XGBRegressor is {'n_estimators': 10}\n",
      "\n",
      "\n",
      "Best model based on R2 is XGBRegressor with R2 of 0.47\n",
      "Best Parameters: {'n_estimators': 10}\n",
      "Best model based on MAE is KNeighborsRegressor with MAE of 0.62\n",
      "Best Parameters: {'n_neighbors': 17}\n",
      "Best model based on MSE is XGBRegressor with MSE of 0.66\n",
      "Best Parameters: {'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Best Parameter for each model\n",
    "for model in sorted_models:\n",
    "    print('Best Parameters for', f\"{model[0]} is {model[4]}\")\n",
    "print('\\n')\n",
    "# Selecting the best model based on R2\n",
    "best_r2_model = max(model_scores, key=lambda x: x[1])\n",
    "print('Best model based on R2 is', f\"{best_r2_model[0]} with R2 of {best_r2_model[1]:.2f}\")\n",
    "print(f'Best Parameters: {best_r2_model[4]}')\n",
    "# Selecting the best model based on MAE\n",
    "best_mae_model = min(model_scores, key=lambda x: x[2])\n",
    "print('Best model based on MAE is', f\"{best_mae_model[0]} with MAE of {best_mae_model[2]:.2f}\")\n",
    "print(f'Best Parameters: {best_mae_model[4]}')\n",
    "# Selecting the best model based on MSE \n",
    "best_mse_model = min(model_scores, key=lambda x: x[3])\n",
    "print('Best model based on MSE is', f\"{best_mse_model[0]} with MSE of {best_mse_model[3]:.2f}\")\n",
    "print(f'Best Parameters: {best_mse_model[4]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Saving Best Model and Predicting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.5676944], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting the best model based on R2\n",
    "best_r2_model = max(model_scores, key=lambda x: x[1])\n",
    "best_model_name = best_r2_model[0]\n",
    "best_model_instance = models[best_model_name][0]\n",
    "print(best_model_name)\n",
    "# export \n",
    "pickle.dump(best_model_instance,open('pipe.pkl','wb'))\n",
    "# Load Model \n",
    "svr = pickle.load(open('pipe.pkl','rb'))\n",
    "# Fit Model \n",
    "svr.fit(X_train, y_train)\n",
    "# Assume user input\n",
    "test_input2 = np.array([15, 1, 0, 3, 1, 1],dtype=object).reshape(1,6)\n",
    "# Predicting the output\n",
    "svr.predict(test_input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsRegressor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.11])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting the best model based on MAE\n",
    "best_mae_model = min(model_scores, key=lambda x: x[2])\n",
    "best_model_name = best_mae_model[0]\n",
    "best_model_instance = models[best_model_name][0]\n",
    "print(best_model_name)\n",
    "# export \n",
    "pickle.dump(best_model_instance,open('pipe.pkl','wb'))\n",
    "# Load Model \n",
    "svr = pickle.load(open('pipe.pkl','rb'))\n",
    "# Fit Model \n",
    "svr.fit(X_train, y_train)\n",
    "# Assume user input\n",
    "test_input2 = np.array([15, 1, 0, 3, 1, 1],dtype=object).reshape(1,6)\n",
    "# Predicting the output\n",
    "svr.predict(test_input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.5676944], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting the best model based on MSE\n",
    "best_mse_model = min(model_scores, key=lambda x: x[3])\n",
    "best_model_name = best_mse_model[0]\n",
    "best_model_instance = models[best_model_name][0]\n",
    "print(best_model_name)\n",
    "# export \n",
    "pickle.dump(best_model_instance,open('pipe.pkl','wb'))\n",
    "# Load Model \n",
    "kn = pickle.load(open('pipe.pkl','rb'))\n",
    "# Fit Model \n",
    "kn.fit(X_train, y_train)\n",
    "# Assume user input\n",
    "test_input2 = np.array([15, 1, 0, 3, 1, 1],dtype=object).reshape(1,6)\n",
    "# Predicting the output\n",
    "kn.predict(test_input2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using GridSeachCv The Best Performing Model based on `MAE and R2 ` is `SVR` and One Basis Of MSE `XGBRegressor is Best ` On `Tips` Dataset \n",
    "- Best Parameter For `SVR` is `{'Kernel': 'poly}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2.Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = sns.load_dataset('diamonds')\n",
    "df = df.sample(1000)\n",
    "# select features and variables\n",
    "X = df.drop('cut', axis=1)\n",
    "y = df['cut']\n",
    "\n",
    "# label encode categorical variables\n",
    "le = LabelEncoder()\n",
    "X['color'] = le.fit_transform(X['color'])\n",
    "X['clarity'] = le.fit_transform(X['clarity'])\n",
    "# encode y \n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "# split the data into train and test data with 80% training dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 RandomizedCvSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score for KNeighborsClassifier is  0.36\n",
      "Accuracy_score for SVC is  0.40\n",
      "Accuracy_score for LogicRegression is  0.40\n",
      "Accuracy_score for AdaBoostClassifier is  0.55\n",
      "Accuracy_score for DesicionTreeClassifier is  0.70\n",
      "Accuracy_score for RandomForestClassifier is  0.71\n",
      "Accuracy_score for GradientBoostingClassifier is  0.72\n",
      "Accuracy_score for XGBClassifier is  0.72\n",
      "\n",
      "\n",
      "F1_Score for KNeighborsClassifier is  0.17\n",
      "F1_Score for SVC is  0.13\n",
      "F1_Score for LogicRegression is  0.16\n",
      "F1_Score for AdaBoostClassifier is  0.45\n",
      "F1_Score for DesicionTreeClassifier is  0.71\n",
      "F1_Score for RandomForestClassifier is  0.73\n",
      "F1_Score for GradientBoostingClassifier is  0.74\n",
      "F1_Score for XGBClassifier is  0.71\n",
      "\n",
      "\n",
      "Precision for KNeighborsClassifier is  0.18\n",
      "Precision for SVC is  0.16\n",
      "Precision for LogicRegression is  0.15\n",
      "Precision for AdaBoostClassifier is  0.41\n",
      "Precision for DesicionTreeClassifier is  0.75\n",
      "Precision for RandomForestClassifier is  0.77\n",
      "Precision for GradientBoostingClassifier is  0.79\n",
      "Precision for XGBClassifier is  0.71\n",
      "CPU times: total: 34.7 s\n",
      "Wall time: 23.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # Create a dictionaries of list of models to evaluate performance with hyperparameters\n",
    "models = { \n",
    "          'LogicRegression' : (LogisticRegression(), {}),\n",
    "          'SVC' : (SVC(), {'kernel': ['rbf', 'poly', 'sigmoid']}),\n",
    "          'DesicionTreeClassifier' : (DecisionTreeClassifier(random_state=42), {'max_depth': [None, 5, 10],'random_state': [42]}),\n",
    "          'RandomForestClassifier' : (RandomForestClassifier(random_state=42), {'n_estimators': [10, 100],'random_state': [42],'max_depth': [None, 5, 10]}),\n",
    "          'KNeighborsClassifier' : (KNeighborsClassifier(), {'n_neighbors': np.arange(3, 100, 2),}),\n",
    "          'GradientBoostingClassifier' : (GradientBoostingClassifier(random_state=42),{'n_estimators': [10, 100],'random_state': [42]}),\n",
    "          'XGBClassifier' : (XGBClassifier(), {'n_estimators': [10, 100]}),  \n",
    "          'AdaBoostClassifier': (AdaBoostClassifier(random_state=42), {'n_estimators': [10, 100],'random_state': [42]}),\n",
    "          }\n",
    "\n",
    "# train and predict each model with evaluation metrics as well making a for loop to iterate over the models\n",
    "model_scores = []\n",
    "for name, (model, params) in models.items():\n",
    "    # create a pipline\n",
    "    pipeline =RandomizedSearchCV(model, params, cv=5)\n",
    "    \n",
    "    # fit the pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # make prediction from each model\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    # Metric\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred,average='macro')\n",
    "    precision = precision_score(y_test, y_pred,average='macro')\n",
    "    best_parameter = pipeline.best_params_\n",
    "    # Append these in the model_scores\n",
    "    model_scores.append((name,accuracy , f1 , precision,best_parameter))                                    \n",
    "\n",
    "# selecting the best model from all above models with evaluation metrics sorting method\n",
    "sorted_models = sorted(model_scores, key=lambda x: x[1])\n",
    "\n",
    "# Printing Each model with evaluation metrics\n",
    "for model in sorted_models:\n",
    "    print('Accuracy_score for', f\"{model[0]} is {model[1]: .2f}\")\n",
    "print('\\n')\n",
    "for model in sorted_models:\n",
    "    print('F1_Score for', f\"{model[0]} is {model[2]: .2f}\")\n",
    "print('\\n')\n",
    "for model in sorted_models:\n",
    "    print('Precision for', f\"{model[0]} is {model[3]: .2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best Parameter and Best Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for KNeighborsClassifier is {'n_neighbors': 29}\n",
      "Best Parameters for SVC is {'kernel': 'poly'}\n",
      "Best Parameters for LogicRegression is {}\n",
      "Best Parameters for AdaBoostClassifier is {'random_state': 42, 'n_estimators': 10}\n",
      "Best Parameters for DesicionTreeClassifier is {'random_state': 42, 'max_depth': 5}\n",
      "Best Parameters for RandomForestClassifier is {'random_state': 42, 'n_estimators': 100, 'max_depth': 10}\n",
      "Best Parameters for GradientBoostingClassifier is {'random_state': 42, 'n_estimators': 10}\n",
      "Best Parameters for XGBClassifier is {'n_estimators': 10}\n",
      "\n",
      "\n",
      "Best model based on Accuracy is GradientBoostingClassifier with Accuracy of 0.72\n",
      "Best model based on F1_Score is GradientBoostingClassifier with F1_Score of 0.74\n",
      "Best model based on Precision is GradientBoostingClassifier with Precision of 0.79\n"
     ]
    }
   ],
   "source": [
    "# Best Parameter for each model\n",
    "for model in sorted_models:\n",
    "    print('Best Parameters for', f\"{model[0]} is {model[4]}\")\n",
    "print('\\n')\n",
    "# Selecting the best model based on MAE\n",
    "best_Accuracy_model = max(model_scores, key=lambda x: x[1])\n",
    "print('Best model based on Accuracy is', f\"{best_Accuracy_model[0]} with Accuracy of {best_Accuracy_model[1]:.2f}\")\n",
    "\n",
    "# Selecting the best model based on R2\n",
    "best_f1_model = max(model_scores, key=lambda x: x[2])\n",
    "print('Best model based on F1_Score is', f\"{best_f1_model[0]} with F1_Score of {best_f1_model[2]:.2f}\")\n",
    "\n",
    "# Selecting the best model based on MSE \n",
    "best_Precision_model = max(model_scores, key=lambda x: x[3])\n",
    "print('Best model based on Precision is', f\"{best_Precision_model[0]} with Precision of {best_Precision_model[3]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using RandomizedCvSearch The Best Performing Model is `GradientBoostingClassifier`, `AdaBoostClassifier` and `XGBClassifier` On `Diamonds` Dataset \n",
    "- Best Parameter For `GradientBoostingClassifier` is `{'n_estimator': 10}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier\n",
      "Accuracy_score for GradientBoostingClassifier is  0.73\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting Best Model based on Accuracy\n",
    "best_Accuracy_model = max(model_scores, key=lambda x: x[1])\n",
    "best_model_name = best_Accuracy_model[0]\n",
    "best_model_instance = models[best_model_name][0]\n",
    "print(best_model_name)\n",
    "# export\n",
    "pickle.dump(best_model_instance,open('Classifier.pkl','wb'))\n",
    "# Load Model\n",
    "dtc = pickle.load(open('Classifier.pkl','rb'))\n",
    "# Fit Model\n",
    "dtc.fit(X_train, y_train)\n",
    "# Predicting the output\n",
    "y_pred = dtc.predict(X_test)\n",
    "# Accuracy \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy_score for', f\"{best_model_name} is {accuracy: .2f}\")\n",
    "# Assume user input\n",
    "test_input2 = np.array([0.51,0,5,61.5,56.0,1674,5.21,5.17,3.19],dtype=object).reshape(1,9)\n",
    "# Predicting the output\n",
    "dtc.predict(test_input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier\n",
      "f1_score for GradientBoostingClassifier is  0.76\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting Best Model based on F!_Score\n",
    "best_f1_model = max(model_scores, key=lambda x: x[2])\n",
    "best_model_name = best_f1_model[0]\n",
    "best_model_instance = models[best_model_name][0]\n",
    "print(best_model_name)\n",
    "# export\n",
    "pickle.dump(best_model_instance,open('Classifier_f1.pkl','wb'))\n",
    "# Load Model\n",
    "dtc = pickle.load(open('Classifier_f1.pkl','rb'))\n",
    "# Fit Model\n",
    "dtc.fit(X_train, y_train)\n",
    "# Predicting the output\n",
    "y_pred = dtc.predict(X_test)\n",
    "# f1 \n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print('f1_score for', f\"{best_model_name} is {f1: .2f}\")\n",
    "# Assume user input\n",
    "test_input2 = np.array([0.51,0,5,61.5,56.0,1674,5.21,5.17,3.19],dtype=object).reshape(1,9)\n",
    "# Predicting the output\n",
    "dtc.predict(test_input2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score for KNeighborsClassifier is  0.39\n",
      "Accuracy_score for SVC is  0.40\n",
      "Accuracy_score for LogicRegression is  0.40\n",
      "Accuracy_score for AdaBoostClassifier is  0.55\n",
      "Accuracy_score for DesicionTreeClassifier is  0.70\n",
      "Accuracy_score for RandomForestClassifier is  0.71\n",
      "Accuracy_score for GradientBoostingClassifier is  0.72\n",
      "Accuracy_score for XGBClassifier is  0.72\n",
      "\n",
      "\n",
      "F1_Score for KNeighborsClassifier is  0.19\n",
      "F1_Score for SVC is  0.13\n",
      "F1_Score for LogicRegression is  0.16\n",
      "F1_Score for AdaBoostClassifier is  0.45\n",
      "F1_Score for DesicionTreeClassifier is  0.71\n",
      "F1_Score for RandomForestClassifier is  0.73\n",
      "F1_Score for GradientBoostingClassifier is  0.74\n",
      "F1_Score for XGBClassifier is  0.71\n",
      "\n",
      "\n",
      "Precision for KNeighborsClassifier is  0.27\n",
      "Precision for SVC is  0.16\n",
      "Precision for LogicRegression is  0.15\n",
      "Precision for AdaBoostClassifier is  0.41\n",
      "Precision for DesicionTreeClassifier is  0.75\n",
      "Precision for RandomForestClassifier is  0.77\n",
      "Precision for GradientBoostingClassifier is  0.79\n",
      "Precision for XGBClassifier is  0.71\n",
      "CPU times: total: 44.4 s\n",
      "Wall time: 45.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # Create a dictionaries of list of models to evaluate performance with hyperparameters\n",
    "models = { \n",
    "          'LogicRegression' : (LogisticRegression(), {}),\n",
    "          'SVC' : (SVC(), {'kernel': ['rbf', 'poly', 'sigmoid']}),\n",
    "          'DesicionTreeClassifier' : (DecisionTreeClassifier(random_state=42), {'max_depth': [None, 5, 10],'random_state': [42]}),\n",
    "          'RandomForestClassifier' : (RandomForestClassifier(random_state=42), {'n_estimators': [10, 100],'random_state': [42],'max_depth': [None, 5, 10]}),\n",
    "          'KNeighborsClassifier' : (KNeighborsClassifier(), {'n_neighbors': np.arange(3, 100, 2),}),\n",
    "          'GradientBoostingClassifier' : (GradientBoostingClassifier(random_state=42),{'n_estimators': [10, 100],'random_state': [42]}),\n",
    "          'XGBClassifier' : (XGBClassifier(), {'n_estimators': [10, 100]}),  \n",
    "          'AdaBoostClassifier': (AdaBoostClassifier(random_state=42), {'n_estimators': [10, 100],'random_state': [42]}),\n",
    "          }\n",
    "\n",
    "# train and predict each model with evaluation metrics as well making a for loop to iterate over the models\n",
    "model_scores = []\n",
    "for name, (model, params) in models.items():\n",
    "    # create a pipline\n",
    "    pipeline =GridSearchCV(model, params, cv=5)\n",
    "    \n",
    "    # fit the pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # make prediction from each model\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    # Metric\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred,average='macro')\n",
    "    precision = precision_score(y_test, y_pred,average='macro')\n",
    "    best_parameter = pipeline.best_params_\n",
    "    # Append these in the model_scores\n",
    "    model_scores.append((name,accuracy , f1 , precision,best_parameter))\n",
    "\n",
    "# selecting the best model from all above models with evaluation metrics sorting method\n",
    "sorted_models = sorted(model_scores, key=lambda x: x[1])\n",
    "\n",
    "# Printing Each model with evaluation metrics\n",
    "for model in sorted_models:\n",
    "    print('Accuracy_score for', f\"{model[0]} is {model[1]: .2f}\")\n",
    "print('\\n')\n",
    "for model in sorted_models:\n",
    "    print('F1_Score for', f\"{model[0]} is {model[2]: .2f}\")\n",
    "print('\\n')\n",
    "for model in sorted_models:\n",
    "    print('Precision for', f\"{model[0]} is {model[3]: .2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best Parameter and Best Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for KNeighborsClassifier is {'n_neighbors': 25}\n",
      "Best Parameters for SVC is {'kernel': 'poly'}\n",
      "Best Parameters for LogicRegression is {}\n",
      "Best Parameters for AdaBoostClassifier is {'n_estimators': 10, 'random_state': 42}\n",
      "Best Parameters for DesicionTreeClassifier is {'max_depth': 5, 'random_state': 42}\n",
      "Best Parameters for RandomForestClassifier is {'max_depth': 10, 'n_estimators': 100, 'random_state': 42}\n",
      "Best Parameters for GradientBoostingClassifier is {'n_estimators': 10, 'random_state': 42}\n",
      "Best Parameters for XGBClassifier is {'n_estimators': 10}\n",
      "\n",
      "\n",
      "Best model based on Accuracy is GradientBoostingClassifier with Accuracy of 0.72\n",
      "Best model based on F1_Score is GradientBoostingClassifier with F1_Score of 0.74\n",
      "Best model based on Precision is GradientBoostingClassifier with Precision of 0.79\n"
     ]
    }
   ],
   "source": [
    "# Best Parameter for each model\n",
    "for model in sorted_models:\n",
    "    print('Best Parameters for', f\"{model[0]} is {model[4]}\")\n",
    "print('\\n')\n",
    "# Selecting the best model based on MAE\n",
    "best_Accuracy_model = max(model_scores, key=lambda x: x[1])\n",
    "print('Best model based on Accuracy is', f\"{best_Accuracy_model[0]} with Accuracy of {best_Accuracy_model[1]:.2f}\")\n",
    "\n",
    "# Selecting the best model based on R2\n",
    "best_f1_model = max(model_scores, key=lambda x: x[2])\n",
    "print('Best model based on F1_Score is', f\"{best_f1_model[0]} with F1_Score of {best_f1_model[2]:.2f}\")\n",
    "\n",
    "# Selecting the best model based on MSE \n",
    "best_Precision_model = max(model_scores, key=lambda x: x[3])\n",
    "print('Best model based on Precision is', f\"{best_Precision_model[0]} with Precision of {best_Precision_model[3]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using GridSreachCv The Best Performing Model is `GradientBoostingClassifier` ,`AdaBoostClassifier` and `XGBClassifier` On `Diamonds` Dataset \n",
    "- Best Parameters for XGBClassifier is {'n_estimators': 10}\n",
    "- Best Parameters for GradientBoostingClassifier is {'n_estimators': 10}\n",
    "- Best Parameters for AdaBoostClassifier is {'n_estimators': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute error for SVR is  0.57\n",
      "Mean Absolute error for LinearRegression is  0.67\n",
      "Mean Absolute error for XGBRegressor is  0.67\n",
      "Mean Absolute error for GradientBoostingRegressor is  0.73\n",
      "Mean Absolute error for KNeighborsRegressor is  0.73\n",
      "Mean Absolute error for RandomForestRegressor is  0.78\n",
      "Mean Absolute error for AdaBoostRegressor is  0.81\n",
      "Mean Absolute error for DecisionTreeRegressor is  0.92\n",
      "===============================================================\n",
      "R2 for SVR is  0.57\n",
      "R2 for LinearRegression is  0.44\n",
      "R2 for XGBRegressor is  0.41\n",
      "R2 for GradientBoostingRegressor is  0.36\n",
      "R2 for KNeighborsRegressor is  0.33\n",
      "R2 for RandomForestRegressor is  0.23\n",
      "R2 for AdaBoostRegressor is  0.21\n",
      "R2 for DecisionTreeRegressor is -0.13\n",
      "===============================================================\n",
      "MSE for SVR is  0.54\n",
      "MSE for LinearRegression is  0.69\n",
      "MSE for XGBRegressor is  0.74\n",
      "MSE for GradientBoostingRegressor is  0.80\n",
      "MSE for KNeighborsRegressor is  0.84\n",
      "MSE for RandomForestRegressor is  0.96\n",
      "MSE for AdaBoostRegressor is  0.99\n",
      "MSE for DecisionTreeRegressor is  1.41\n",
      "===============================================================\n",
      "Best model based on MAE is SVR with MAE of 0.57\n",
      "Best model based on R2 is SVR with R2 of 0.57\n",
      "Best model based on MSE is SVR with MSE of 0.54\n"
     ]
    }
   ],
   "source": [
    "df = sns.load_dataset('tips')\n",
    "\n",
    "# select features and variables\n",
    "X = df.drop('tip', axis=1)\n",
    "y = df['tip']\n",
    "\n",
    "# label encode categorical variables\n",
    "le = LabelEncoder()\n",
    "X['sex'] = le.fit_transform(X['sex'])\n",
    "X['smoker'] = le.fit_transform(X['smoker'])\n",
    "X['day'] = le.fit_transform(X['day'])\n",
    "X['time'] = le.fit_transform(X['time'])\n",
    "\n",
    "# split the data into train and test data with 80% training dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a dictionaries of list of models to evaluate performance\n",
    "models = { \n",
    "          'LinearRegression' : LinearRegression(),\n",
    "          'SVR' : SVR(),\n",
    "          'DecisionTreeRegressor' : DecisionTreeRegressor(random_state=42),\n",
    "          'RandomForestRegressor' : RandomForestRegressor(random_state=42),\n",
    "          'KNeighborsRegressor' : KNeighborsRegressor(),\n",
    "          'GradientBoostingRegressor' : GradientBoostingRegressor(random_state=42),\n",
    "          'XGBRegressor' : XGBRegressor()  ,\n",
    "          'AdaBoostRegressor' : AdaBoostRegressor(random_state=42)       \n",
    "          }\n",
    "\n",
    "# train and predict each model with evaluation metrics as well making a for loop to iterate over the models\n",
    "\n",
    "model_scores = []\n",
    "for name, model in models.items():\n",
    "    # fit each model from models on training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # make prediction from each model\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    # metric = r2_score(y_test, y_pred)\n",
    "    model_scores.append((name, mae , r2 , mse))\n",
    "    \n",
    "      \n",
    "    # # print the performing metric\n",
    "    # print(name, 'MSE: ', mean_squared_error(y_test, y_pred))\n",
    "    # print(name, 'R2: ', r2_score(y_test, y_pred))\n",
    "    # print(name, 'MAE: ', mean_absolute_error(y_test, y_pred))\n",
    "    # print('\\n')\n",
    "# selecting the best model from all above models with evaluation metrics sorting method\n",
    "sorted_models = sorted(model_scores, key=lambda x: x[1], reverse=False)\n",
    "for model in sorted_models:\n",
    "    print('Mean Absolute error for', f\"{model[0]} is {model[1]: .2f}\")\n",
    "print('===============================================================')\n",
    "for model in sorted_models:\n",
    "    print('R2 for', f\"{model[0]} is {model[2]: .2f}\")\n",
    "print('===============================================================')\n",
    "\n",
    "for model in sorted_models:\n",
    "    print('MSE for', f\"{model[0]} is {model[3]: .2f}\") \n",
    "print('===============================================================')\n",
    "\n",
    "# Selecting the best model based on MAE\n",
    "best_mae_model = min(model_scores, key=lambda x: x[1])\n",
    "print('Best model based on MAE is', f\"{best_mae_model[0]} with MAE of {best_mae_model[1]:.2f}\")\n",
    "\n",
    "# Selecting the best model based on R2\n",
    "best_r2_model = max(model_scores, key=lambda x: x[2])\n",
    "print('Best model based on R2 is', f\"{best_r2_model[0]} with R2 of {best_r2_model[2]:.2f}\")\n",
    "\n",
    "# Selecting the best model based on MSE \n",
    "best_mse_model = min(model_scores, key=lambda x: x[3])\n",
    "print('Best model based on MSE is', f\"{best_mse_model[0]} with MSE of {best_mse_model[3]:.2f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
