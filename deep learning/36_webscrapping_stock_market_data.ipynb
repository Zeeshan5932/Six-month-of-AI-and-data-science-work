{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:yellow;font-weight:bold;text-decoration:underline; font-size:50px\">Stock market data scrapping</span> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why stock market data scrapping is important?\n",
    "Stock market data scraping is important because it allows traders and investors to gather large amounts of data from various sources, analyze it, and make informed decisions based on the insights gained. This can include tracking stock prices, analyzing market trends, and monitoring news and social media sentiment. Additionally, stock market data scraping can help traders and investors identify potential risks and opportunities, and make more informed decisions about when to buy or sell stocks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several libraries that can be used to scrape stock market data in Python. Some popular ones include:\n",
    "\n",
    "1. BeautifulSoup\n",
    "2. Scrapy\n",
    "3. Selenium\n",
    "4. Pandas\n",
    "5. Requests\n",
    "\n",
    "Each of these libraries has its own strengths and weaknesses, and the choice of which one to use will depend on the specific requirements of your project. For example, if you need to scrape data from dynamic websites that require user interaction, you may want to use Selenium. If you need to scrape data from multiple pages or websites, Scrapy may be a good choice. If you need to manipulate and analyze the data after scraping it, Pandas may be the way to go."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Code for Scraping Stock Prices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of how to use yfinance to get the stock price of Apple (AAPL):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Downloading yfinance-0.2.36-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\zeesh\\appdata\\local\\miniconda3\\envs\\tensor_flow\\lib\\site-packages (from yfinance) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\zeesh\\appdata\\local\\miniconda3\\envs\\tensor_flow\\lib\\site-packages (from yfinance) (1.26.3)\n",
      "Collecting requests>=2.31 (from yfinance)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting multitasking>=0.0.7 (from yfinance)\n",
      "  Downloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Collecting lxml>=4.9.1 (from yfinance)\n",
      "  Downloading lxml-5.1.0-cp312-cp312-win_amd64.whl.metadata (3.6 kB)\n",
      "Collecting appdirs>=1.4.4 (from yfinance)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\zeesh\\appdata\\local\\miniconda3\\envs\\tensor_flow\\lib\\site-packages (from yfinance) (2024.1)\n",
      "Collecting frozendict>=2.3.4 (from yfinance)\n",
      "  Downloading frozendict-2.4.0.tar.gz (314 kB)\n",
      "     ---------------------------------------- 0.0/314.6 kB ? eta -:--:--\n",
      "     --- ----------------------------------- 30.7/314.6 kB 1.3 MB/s eta 0:00:01\n",
      "     ------- ----------------------------- 61.4/314.6 kB 825.8 kB/s eta 0:00:01\n",
      "     -------------- --------------------- 122.9/314.6 kB 901.1 kB/s eta 0:00:01\n",
      "     --------------------------- ---------- 225.3/314.6 kB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------  307.2/314.6 kB 1.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ 314.6/314.6 kB 975.4 kB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting peewee>=3.16.2 (from yfinance)\n",
      "  Downloading peewee-3.17.1.tar.gz (3.0 MB)\n",
      "     ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.2/3.0 MB 5.9 MB/s eta 0:00:01\n",
      "     ---- ----------------------------------- 0.3/3.0 MB 2.9 MB/s eta 0:00:01\n",
      "     ----- ---------------------------------- 0.4/3.0 MB 2.3 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.5/3.0 MB 2.4 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.6/3.0 MB 2.2 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 0.7/3.0 MB 2.2 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 0.8/3.0 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.8/3.0 MB 2.0 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 0.9/3.0 MB 1.8 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 1.0/3.0 MB 1.8 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 1.1/3.0 MB 1.8 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 1.1/3.0 MB 1.8 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 1.2/3.0 MB 1.8 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 1.3/3.0 MB 1.8 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 1.3/3.0 MB 1.7 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 1.4/3.0 MB 1.7 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 1.5/3.0 MB 1.7 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 1.6/3.0 MB 1.7 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 1.7/3.0 MB 1.8 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 1.7/3.0 MB 1.8 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 1.7/3.0 MB 1.8 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.8/3.0 MB 1.6 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.9/3.0 MB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 2.0/3.0 MB 1.7 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 2.1/3.0 MB 1.7 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 2.2/3.0 MB 1.7 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 2.3/3.0 MB 1.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 2.4/3.0 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 2.6/3.0 MB 1.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 2.8/3.0 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.9/3.0 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.0/3.0 MB 1.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting beautifulsoup4>=4.11.1 (from yfinance)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting html5lib>=1.1 (from yfinance)\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "     ---------------------------------------- 0.0/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 30.7/112.2 kB ? eta -:--:--\n",
      "     ------------- ------------------------ 41.0/112.2 kB 25.9 kB/s eta 0:00:03\n",
      "     ------------- ------------------------ 41.0/112.2 kB 25.9 kB/s eta 0:00:03\n",
      "     ------------- ------------------------ 41.0/112.2 kB 25.9 kB/s eta 0:00:03\n",
      "     ------------- ------------------------ 41.0/112.2 kB 25.9 kB/s eta 0:00:03\n",
      "     ------------------------------------- 112.2/112.2 kB 73.3 kB/s eta 0:00:00\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4>=4.11.1->yfinance)\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\zeesh\\appdata\\local\\miniconda3\\envs\\tensor_flow\\lib\\site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Collecting webencodings (from html5lib>=1.1->yfinance)\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zeesh\\appdata\\local\\miniconda3\\envs\\tensor_flow\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\zeesh\\appdata\\local\\miniconda3\\envs\\tensor_flow\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2023.4)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.31->yfinance)\n",
      "  Downloading charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.31->yfinance)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.31->yfinance)\n",
      "  Using cached urllib3-2.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.31->yfinance)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Downloading yfinance-0.2.36-py2.py3-none-any.whl (72 kB)\n",
      "   ---------------------------------------- 0.0/72.4 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 10.2/72.4 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 10.2/72.4 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 30.7/72.4 kB 262.6 kB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 41.0/72.4 kB 217.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 61.4/72.4 kB 272.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 72.4/72.4 kB 283.8 kB/s eta 0:00:00\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "   ---------------------------------------- 0.0/147.9 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/147.9 kB ? eta -:--:--\n",
      "   ---------------- ---------------------- 61.4/147.9 kB 825.8 kB/s eta 0:00:01\n",
      "   ------------------ -------------------- 71.7/147.9 kB 787.7 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 112.6/147.9 kB 731.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- 147.9/147.9 kB 678.3 kB/s eta 0:00:00\n",
      "Downloading lxml-5.1.0-cp312-cp312-win_amd64.whl (3.9 MB)\n",
      "   ---------------------------------------- 0.0/3.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/3.9 MB 3.5 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.2/3.9 MB 2.3 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.2/3.9 MB 1.7 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.2/3.9 MB 1.4 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.2/3.9 MB 1.4 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.3/3.9 MB 1.2 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.4/3.9 MB 1.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.4/3.9 MB 1.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.4/3.9 MB 981.2 kB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.5/3.9 MB 992.3 kB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.5/3.9 MB 1.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.6/3.9 MB 1.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.7/3.9 MB 1.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.7/3.9 MB 990.5 kB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.7/3.9 MB 990.5 kB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.7/3.9 MB 990.5 kB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.7/3.9 MB 990.5 kB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.7/3.9 MB 990.5 kB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.7/3.9 MB 990.5 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.7/3.9 MB 719.2 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.7/3.9 MB 719.2 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.7/3.9 MB 665.1 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.7/3.9 MB 645.8 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.7/3.9 MB 637.2 kB/s eta 0:00:05\n",
      "   ------- -------------------------------- 0.8/3.9 MB 629.2 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 0.8/3.9 MB 621.9 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 0.8/3.9 MB 614.7 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 0.8/3.9 MB 617.1 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 0.9/3.9 MB 624.9 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.9/3.9 MB 620.4 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 1.0/3.9 MB 635.0 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 1.0/3.9 MB 642.0 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 1.1/3.9 MB 655.4 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 1.1/3.9 MB 667.4 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 1.2/3.9 MB 673.1 kB/s eta 0:00:05\n",
      "   ------------ --------------------------- 1.2/3.9 MB 684.6 kB/s eta 0:00:04\n",
      "   ------------ --------------------------- 1.2/3.9 MB 684.6 kB/s eta 0:00:04\n",
      "   ------------ --------------------------- 1.3/3.9 MB 683.0 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 1.3/3.9 MB 693.1 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 1.4/3.9 MB 703.3 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 1.4/3.9 MB 706.6 kB/s eta 0:00:04\n",
      "   -------------- ------------------------- 1.5/3.9 MB 705.3 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 1.5/3.9 MB 719.1 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 1.6/3.9 MB 727.6 kB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 1.6/3.9 MB 727.0 kB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 1.6/3.9 MB 738.4 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 1.7/3.9 MB 742.2 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 1.7/3.9 MB 748.2 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 1.8/3.9 MB 745.9 kB/s eta 0:00:03\n",
      "   ------------------ --------------------- 1.8/3.9 MB 758.0 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 1.9/3.9 MB 763.6 kB/s eta 0:00:03\n",
      "   ------------------- -------------------- 1.9/3.9 MB 770.0 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 2.0/3.9 MB 784.6 kB/s eta 0:00:03\n",
      "   -------------------- ------------------- 2.0/3.9 MB 786.2 kB/s eta 0:00:03\n",
      "   --------------------- ------------------ 2.1/3.9 MB 791.0 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 2.2/3.9 MB 804.8 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 2.2/3.9 MB 808.7 kB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 2.3/3.9 MB 820.8 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 2.4/3.9 MB 828.9 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 2.4/3.9 MB 833.4 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 2.5/3.9 MB 841.1 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 2.5/3.9 MB 845.0 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 2.6/3.9 MB 854.5 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 2.6/3.9 MB 851.4 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 2.7/3.9 MB 865.1 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 2.8/3.9 MB 865.2 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 2.9/3.9 MB 874.8 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 3.0/3.9 MB 890.2 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 3.0/3.9 MB 885.9 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 3.1/3.9 MB 897.7 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.2/3.9 MB 906.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.9 MB 913.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.3/3.9 MB 918.4 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.4/3.9 MB 926.5 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.4/3.9 MB 935.2 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.5/3.9 MB 940.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.6/3.9 MB 947.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.7/3.9 MB 956.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.7/3.9 MB 960.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.8/3.9 MB 969.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.9/3.9 MB 975.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.9/3.9 MB 971.5 kB/s eta 0:00:00\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Downloading charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
      "   ---------------------------------------- 0.0/100.4 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 92.2/100.4 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 100.4/100.4 kB 1.2 MB/s eta 0:00:00\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Using cached urllib3-2.2.0-py3-none-any.whl (120 kB)\n",
      "Building wheels for collected packages: frozendict, peewee\n",
      "  Building wheel for frozendict (pyproject.toml): started\n",
      "  Building wheel for frozendict (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for frozendict: filename=frozendict-2.4.0-py3-none-any.whl size=15471 sha256=e985794b324a869f6e11978912aaa295c8e203784089201f9c2a29e847c28485\n",
      "  Stored in directory: c:\\users\\zeesh\\appdata\\local\\pip\\cache\\wheels\\f8\\24\\61\\a7327cee6da6ddddd619cddf13acfb4ec63606c69bbfe31486\n",
      "  Building wheel for peewee (pyproject.toml): started\n",
      "  Building wheel for peewee (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for peewee: filename=peewee-3.17.1-py3-none-any.whl size=136948 sha256=7bd4a4c0de418a3f47632d0e22c1f1fa1d9e417bb2a4cfe55020936e18b8b85c\n",
      "  Stored in directory: c:\\users\\zeesh\\appdata\\local\\pip\\cache\\wheels\\fe\\58\\f4\\132ded03aa364dd5d2bbc2d02e1c2074a9b5012776d15fc2dc\n",
      "Successfully built frozendict peewee\n",
      "Installing collected packages: webencodings, peewee, multitasking, appdirs, urllib3, soupsieve, lxml, idna, html5lib, frozendict, charset-normalizer, certifi, requests, beautifulsoup4, yfinance\n",
      "Successfully installed appdirs-1.4.4 beautifulsoup4-4.12.3 certifi-2024.2.2 charset-normalizer-3.3.2 frozendict-2.4.0 html5lib-1.1 idna-3.6 lxml-5.1.0 multitasking-0.0.11 peewee-3.17.1 requests-2.31.0 soupsieve-2.5 urllib3-2.2.0 webencodings-0.5.1 yfinance-0.2.36\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176.18692016601562\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Define the ticker symbol\n",
    "tickerSymbol = 'AAPL'\n",
    "\n",
    "# Get data on this ticker\n",
    "tickerData = yf.Ticker(tickerSymbol)\n",
    "\n",
    "# Get the stock price history\n",
    "tickerDf = tickerData.history(period='1d', start='2021-01-01', end='2021-12-31')\n",
    "\n",
    "# Get the last closing price\n",
    "lastPrice = tickerDf['Close'].iloc[-1]\n",
    "\n",
    "# Print the last closing price\n",
    "print(lastPrice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This code uses the yfinance library to get data on the AAPL ticker, including its stock price history. It then extracts the last closing price from the history and prints it to the console. Note that you can adjust the `start` and `end` dates to get the stock price history for a different time period."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:yellow;font-weight:bold;text-decoration:underline; font-size:50px\">Web Scrapping vs. web crawling</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Web scraping` and `web crawling` are two related but distinct techniques for gathering data from the web.\n",
    "\n",
    "`Web scraping` involves `extracting specific data from web pages`, typically using tools like `BeautifulSoup` or `Scrapy` in Python. \n",
    "- This data can be used for a variety of purposes, such as analyzing market trends, monitoring social media sentiment, or gathering product information for price comparison websites.\n",
    "\n",
    "`Web crawling`, on the other hand, involves systematically exploring the web to gather data, typically using automated bots or spiders. \n",
    "- This data can be used to create search engine indexes, monitor website changes, or gather data for academic research.\n",
    "\n",
    "In summary, `web scraping is focused on extracting specific data from web pages`, while `web crawling is focused on systematically exploring the web to gather data`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange;font-weight:bold;font-size:30px\">Challenges scrapping stockmarket data</span>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several common challenges when scraping stock market data, including:\n",
    "\n",
    "1. `Dynamic websites:` Many stock market websites use dynamic content that is generated by JavaScript, which can make it difficult to scrape the data using traditional web scraping techniques. In these cases, you may need to use a tool like Selenium to automate a web browser and interact with the website in order to scrape the data.\n",
    "\n",
    "2. `Anti-scraping measures:` Some websites may have anti-scraping measures in place to prevent automated scraping. These measures can include CAPTCHAs, IP blocking, or user agent detection. To avoid these measures, you may need to use techniques like rotating IP addresses or user agents, or using a proxy server.\n",
    "\n",
    "3. `Data formatting:` Stock market data can be presented in a variety of formats, including tables, charts, and graphs. Extracting the data from these formats can be challenging, and may require specialized tools or techniques.\n",
    "\n",
    "4. `Data quality:` Stock market data can be noisy and contain errors or outliers. It's important to carefully clean and validate the data before using it for analysis or decision-making.\n",
    "\n",
    "5. `Legal and ethical considerations:` Scraping stock market data can raise legal and ethical concerns, particularly if the data is used for insider trading or other illegal activities. It's important to ensure that your scraping activities are legal and ethical, and to obtain any necessary permissions or licenses before scraping data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange;font-weight:bold;font-size:30px\">Techniques to avoid anti-scraping measures when scraping stock market data?</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several techniques that can be used to avoid anti-scraping measures when scraping stock market data, including:\n",
    "\n",
    "1. Use a proxy server: A proxy server can be used to route your requests through a different IP address, which can help you avoid IP blocking. There are several free and paid proxy services available that you can use.\n",
    "\n",
    "2. Rotate user agents: Some websites may block requests from certain user agents, so rotating your user agent can help you avoid detection. You can use a library like `fake_useragent` in Python to generate random user agents for each request.\n",
    "\n",
    "3. Slow down your requests: Sending too many requests too quickly can trigger anti-scraping measures, so slowing down your requests can help you avoid detection. You can use a library like `time` in Python to add a delay between each request.\n",
    "\n",
    "4. Use CAPTCHA solving services: Some websites may require you to solve a CAPTCHA in order to access the data. There are several CAPTCHA solving services available that you can use to automate this process.\n",
    "\n",
    "5. Use headless browsers: Some websites may use JavaScript to generate content, which can make it difficult to scrape the data using traditional web scraping techniques. Using a headless browser like Selenium can help you automate the process of interacting with the website and scraping the data.\n",
    "\n",
    "It's important to note that while these techniques can help you avoid anti-scraping measures, they may not be foolproof and may still result in your requests being blocked or your IP address being banned. It's always a good idea to check the website's terms of service and to be respectful of their policies when scraping data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange;font-weight:bold;font-size:30px\">What are some common data formatting issues when scraping stock market data?</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several common data formatting issues when scraping stock market data, including:\n",
    "\n",
    "1. `Inconsistent data types:` Stock market data can be presented in a variety of formats, including text, numbers, and dates. It's important to ensure that the data is consistently formatted and that the data types are correct before using it for analysis.\n",
    "\n",
    "2. Missing data: Stock market data can be incomplete or missing, which can make it difficult to analyze. It's important to handle missing data appropriately, either by imputing missing values or by excluding them from the analysis.\n",
    "\n",
    "3. `Non-standard data formats:` Some stock market data may be presented in non-standard formats, such as PDFs or images. Extracting data from these formats can be challenging and may require specialized tools or techniques.\n",
    "\n",
    "4. `Data normalization:` Stock market data can be presented in different units or currencies, which can make it difficult to compare across different stocks or markets. It's important to normalize the data to a common unit or currency before using it for analysis.\n",
    "\n",
    "5. `Data cleaning:` Stock market data can be noisy and contain errors or outliers. It's important to carefully clean and validate the data before using it for analysis or decision-making.\n",
    "\n",
    "It's important to be aware of these formatting issues when scraping stock market data and to take steps to address them before using the data for analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's scrape some stock market data!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Here's an example code snippet that uses yfinance to download the stock data of Google (GOOGL) for the last year from today and stores it in a Pandas DataFrame:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Open        High         Low       Close   \n",
      "Date                                                                        \n",
      "2023-06-14 00:00:00-04:00  123.099998  124.050003  121.449997  123.669998  \\\n",
      "2023-06-15 00:00:00-04:00  123.139999  125.459999  122.400002  125.089996   \n",
      "2023-06-16 00:00:00-04:00  125.930000  126.110001  123.279999  123.529999   \n",
      "2023-06-20 00:00:00-04:00  122.930000  124.570000  122.139999  123.099998   \n",
      "2023-06-21 00:00:00-04:00  122.400002  122.660004  120.099998  120.919998   \n",
      "\n",
      "                             Volume  Dividends  Stock Splits  \n",
      "Date                                                          \n",
      "2023-06-14 00:00:00-04:00  30592300        0.0           0.0  \n",
      "2023-06-15 00:00:00-04:00  35246300        0.0           0.0  \n",
      "2023-06-16 00:00:00-04:00  45514000        0.0           0.0  \n",
      "2023-06-20 00:00:00-04:00  26071300        0.0           0.0  \n",
      "2023-06-21 00:00:00-04:00  19226663        0.0           0.0  \n",
      "                                 Open        High         Low       Close   \n",
      "Date                                                                        \n",
      "2022-06-21 00:00:00-04:00  108.929497  112.489502  108.599998  111.543999  \\\n",
      "2022-06-22 00:00:00-04:00  110.556503  113.346497  110.382500  111.487503   \n",
      "2022-06-23 00:00:00-04:00  112.231499  112.742500  110.500504  112.241997   \n",
      "2022-06-24 00:00:00-04:00  112.995003  118.079002  112.952499  117.974998   \n",
      "2022-06-27 00:00:00-04:00  118.273003  118.579498  115.179497  115.833504   \n",
      "\n",
      "                             Volume  Dividends  Stock Splits  \n",
      "Date                                                          \n",
      "2022-06-21 00:00:00-04:00  47416000        0.0           0.0  \n",
      "2022-06-22 00:00:00-04:00  30774000        0.0           0.0  \n",
      "2022-06-23 00:00:00-04:00  28362000        0.0           0.0  \n",
      "2022-06-24 00:00:00-04:00  41164000        0.0           0.0  \n",
      "2022-06-27 00:00:00-04:00  36420000        0.0           0.0  \n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Define the ticker symbol\n",
    "tickerSymbol = 'GOOGL'\n",
    "\n",
    "# Get data on this ticker\n",
    "tickerData = yf.Ticker(tickerSymbol)\n",
    "\n",
    "# Get the stock price history\n",
    "tickerDf = tickerData.history(period='1y')\n",
    "\n",
    "# Print the last 5 rows of the DataFrame\n",
    "print(tickerDf.tail())\n",
    "print(tickerDf.head())\n",
    "# Save the DataFrame to a CSV file\n",
    "# tickerDf.to_csv('googl_stock_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickerDf.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This code first defines the ticker symbol for Google (GOOGL) and uses yfinance to get data on this ticker. It then uses the `history` method to get the stock price history for the last year from today and stores it in a Pandas DataFrame. Finally, it prints the last 5 rows of the DataFrame and saves the DataFrame to a CSV file named `googl_stock_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Open      High       Low     Close     Volume   \n",
      "Date                                                                           \n",
      "2010-06-29 00:00:00-04:00  1.266667  1.666667  1.169333  1.592667  281494500  \\\n",
      "2010-06-30 00:00:00-04:00  1.719333  2.028000  1.553333  1.588667  257806500   \n",
      "2010-07-01 00:00:00-04:00  1.666667  1.728000  1.351333  1.464000  123282000   \n",
      "2010-07-02 00:00:00-04:00  1.533333  1.540000  1.247333  1.280000   77097000   \n",
      "2010-07-06 00:00:00-04:00  1.333333  1.333333  1.055333  1.074000  103003500   \n",
      "\n",
      "                           Dividends  Stock Splits  \n",
      "Date                                                \n",
      "2010-06-29 00:00:00-04:00        0.0           0.0  \n",
      "2010-06-30 00:00:00-04:00        0.0           0.0  \n",
      "2010-07-01 00:00:00-04:00        0.0           0.0  \n",
      "2010-07-02 00:00:00-04:00        0.0           0.0  \n",
      "2010-07-06 00:00:00-04:00        0.0           0.0  \n",
      "(2410, 7)\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# define ticker symbol\n",
    "tickerSymbol = 'TSLA'\n",
    "\n",
    "# get data on this ticker\n",
    "tickerData = yf.Ticker(tickerSymbol)\n",
    "# tickerData.info\n",
    "\n",
    "# get the historical prices for this ticker\n",
    "tickerDf = tickerData.history(period='1d', start='2010-1-1', end='2020-1-25')\n",
    "\n",
    "# last closing price\n",
    "tickerDf['Close'].iloc[-1]\n",
    "\n",
    "print(tickerDf.head())\n",
    "print(tickerDf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-06-21 00:00:00-04:00</th>\n",
       "      <td>165.929993</td>\n",
       "      <td>167.759995</td>\n",
       "      <td>155.970001</td>\n",
       "      <td>157.050003</td>\n",
       "      <td>50724900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-22 00:00:00-04:00</th>\n",
       "      <td>158.380005</td>\n",
       "      <td>160.789993</td>\n",
       "      <td>155.440002</td>\n",
       "      <td>155.850006</td>\n",
       "      <td>47267800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-23 00:00:00-04:00</th>\n",
       "      <td>156.270004</td>\n",
       "      <td>159.750000</td>\n",
       "      <td>154.250000</td>\n",
       "      <td>158.750000</td>\n",
       "      <td>40499200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-24 00:00:00-04:00</th>\n",
       "      <td>161.729996</td>\n",
       "      <td>170.250000</td>\n",
       "      <td>161.300003</td>\n",
       "      <td>170.160004</td>\n",
       "      <td>68736000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-27 00:00:00-04:00</th>\n",
       "      <td>171.320007</td>\n",
       "      <td>171.750000</td>\n",
       "      <td>168.009995</td>\n",
       "      <td>169.490005</td>\n",
       "      <td>29174600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Open        High         Low       Close   \n",
       "Date                                                                        \n",
       "2022-06-21 00:00:00-04:00  165.929993  167.759995  155.970001  157.050003  \\\n",
       "2022-06-22 00:00:00-04:00  158.380005  160.789993  155.440002  155.850006   \n",
       "2022-06-23 00:00:00-04:00  156.270004  159.750000  154.250000  158.750000   \n",
       "2022-06-24 00:00:00-04:00  161.729996  170.250000  161.300003  170.160004   \n",
       "2022-06-27 00:00:00-04:00  171.320007  171.750000  168.009995  169.490005   \n",
       "\n",
       "                             Volume  Dividends  Stock Splits  \n",
       "Date                                                          \n",
       "2022-06-21 00:00:00-04:00  50724900        0.0           0.0  \n",
       "2022-06-22 00:00:00-04:00  47267800        0.0           0.0  \n",
       "2022-06-23 00:00:00-04:00  40499200        0.0           0.0  \n",
       "2022-06-24 00:00:00-04:00  68736000        0.0           0.0  \n",
       "2022-06-27 00:00:00-04:00  29174600        0.0           0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime as dt\n",
    "from datetime import date, timedelta\n",
    "\n",
    "today = date.today()\n",
    "d1 = today.strftime(\"%Y-%m-%d\")\n",
    "d1\n",
    "d2 = (today - timedelta(days=365)).strftime(\"%Y-%m-%d\")\n",
    "d2\n",
    "start_date = d2\n",
    "end_date = d1\n",
    "\n",
    "# define ticker symbol\n",
    "tickerSymbol = 'META'\n",
    "\n",
    "# get data on this ticker\n",
    "tickerData = yf.Ticker(tickerSymbol)\n",
    "# tickerData.info\n",
    "\n",
    "# get the historical prices for this ticker\n",
    "tickerDf = tickerData.history(period='1d', start=start_date, end=end_date)\n",
    "tickerDf.head()\n",
    "\n",
    "# tickerDf.to_csv('META.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscrapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
